{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SQLContext\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessData(dataset):\n",
    "    \n",
    "    # only keeping successful and failed projects\n",
    "    projects = dataset[(dataset['state'] == 'failed') | (dataset['state'] == 'successful')]\n",
    "    \n",
    "    # deleting unnecesary columns\n",
    "    projects.drop('ID',axis=1,inplace=True)\n",
    "    projects.drop('name',axis=1,inplace=True)\n",
    "    projects.drop('category',axis=1,inplace=True)\n",
    "    projects.drop('currency',axis=1,inplace=True)\n",
    "    projects.drop('goal',axis=1,inplace=True)\n",
    "    projects.drop('pledged',axis=1,inplace=True)\n",
    "    projects.drop('usd pledged',axis=1,inplace=True)\n",
    "    \n",
    "    # Finding length of the project \n",
    "    projects['launched'] = pd.to_datetime(projects['launched']).dt.to_period(\"M\")\n",
    "    projects['deadline'] = pd.to_datetime(projects['deadline']).dt.to_period(\"M\")\n",
    "    \n",
    "    # Creating a new columns with Campaign total months\n",
    "    projects['total_months'] = projects['deadline']- projects['launched']\n",
    "    projects['total_months'] = projects['total_months'].astype(int)\n",
    "    projects.drop('launched',axis=1,inplace=True)\n",
    "    projects.drop('deadline',axis=1,inplace=True)\n",
    "    \n",
    "    # Moving the outcome variable to last and encoding it\n",
    "    state = projects.pop('state')\n",
    "    projects['state']=state\n",
    "    projects['state']= LabelEncoder().fit_transform(projects['state'])\n",
    "    \n",
    "    # One Hot Encoding the categorical features\n",
    "    onehotencoder = ce.OneHotEncoder(cols = ['main_category','country'])\n",
    "    projects = onehotencoder.fit_transform(projects)\n",
    "    \n",
    "    # Avoiding dummy variable trap\n",
    "    projects.drop('main_category_-1',axis=1,inplace=True)\n",
    "    projects.drop('country_-1',axis=1,inplace=True)\n",
    "\n",
    "  \n",
    "    labels = projects.columns\n",
    "\n",
    "    X = projects.iloc[:10000, :-1].values\n",
    "    y = projects.iloc[:10000, -1].values\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # Feature Scaling\n",
    "    sc_X = StandardScaler()\n",
    "    X_train[:,38:] = sc_X.fit_transform(X_train[:,38:])\n",
    "    X_test[:,38:] = sc_X.transform(X_test[:,38:])\n",
    "\n",
    "    training_data = pd.concat([pd.DataFrame(X_train),pd.Series(y_train)],axis=1)\n",
    "    test_data = pd.concat([pd.DataFrame(X_test),pd.Series(y_test)],axis=1)\n",
    "\n",
    "    training_data.columns = labels\n",
    "    test_data.columns= labels\n",
    "    \n",
    "    return training_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Tejaswini\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('ks-projects-201801.csv')\n",
    "training_data, test_data = preProcessData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute sigmoid(x,W)\n",
    "def compute_h(x,W):\n",
    "    x = np.append([1],[x])\n",
    "    #print(x_vector)\n",
    "    #perform dot product of x and W\n",
    "    z=np.dot(x,W)\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute \n",
    "def compute_gradient(point):\n",
    "    #create feature vector x\n",
    "    x_vector = np.append([1],[point[1:]])\n",
    "    #print(x_vector.shape)\n",
    "    #create response vector y\n",
    "    w_vector= point[0]\n",
    "    #print(y_vector.shape)\n",
    "    #perform dot product of x and y\n",
    "    gradient = w_vector * x_vector\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for point in devZipX_Rdd.collect():\n",
    "    #create feature vector x\n",
    " #   x_vector = np.append([1],[point[1:]])\n",
    "  #  print(x_vector.shape)\n",
    "    #create response vector y\n",
    "   # y_vector= point[0]\n",
    "    #print(y_vector.shape)\n",
    "    #perform dot product of x and y\n",
    "    #gradient = y_vector * x_vector\n",
    "    #print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def gradient(X, y, predicted):\n",
    "#    print(predicted.shape[0])\n",
    "#    dw = np.dot(X.T , (predicted - y))\n",
    "#    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pred(X,W):\n",
    "    z = np.dot(X, W)\n",
    "    return sigmoid(z)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ones = np.ones(len(training_data))\n",
    "train_X = training_data.iloc[:,:-1].as_matrix()\n",
    "train_y = training_data.iloc[:,-1].as_matrix()\n",
    "W = np.zeros([train_X.shape[1]+1, 1])\n",
    "#print(train_X.shape)\n",
    "learning_rate = 0.5\n",
    "#print(type(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_XRdd = sqlContext.createDataFrame(train_X)\n",
    "train_XRdd = sc.parallelize(train_X)\n",
    "#train_XRdd.collect()\n",
    "train_yRdd = sc.parallelize(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute h(x,W)\n",
    "#pred_yRdd =train_XRdd.map(lambda x : compute_h(x,W))\n",
    "# combine predicted y value rdd and actual y value rdd\n",
    "#predTrain_Rdd = pred_yRdd.zip(train_yRdd) \n",
    "# compute difference between predicted y and actual y value\n",
    "#devRdd = predTrain_Rdd.map(lambda x : (x[0] - x[1]))\n",
    "#len(devRdd.collect())\n",
    "#train_XRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    #compute h(x,W)\n",
    "    pred_yRdd =train_XRdd.map(lambda x : compute_h(x,W))\n",
    "    # combine predicted y value rdd and actual y value rdd\n",
    "    predTrain_Rdd = pred_yRdd.zip(train_yRdd) \n",
    "    # compute difference between predicted y and actual y value\n",
    "    devRdd = predTrain_Rdd.map(lambda x : (x[0] - x[1]))\n",
    "    # compute gradient : (predicted - y)X\n",
    "    devX_Rdd = devRdd.zip(train_XRdd)\n",
    "    gradRdd = devX_Rdd.map(lambda lst : ('keyA',compute_gradient(lst)))\n",
    "    dwRdd = gradRdd.reduceByKey(lambda x,y :  np.add(x,y))\n",
    "    gradientRdd = dwRdd.mapValues(lambda dw : learning_rate * dw)\n",
    "    #update weights\n",
    "    weightRdd = sc.parallelize(W)\n",
    "    #devRdd.lookup('keyA')[0].shape\n",
    "    dRdd = sc.parallelize(gradientRdd.lookup('keyA')[0])\n",
    "    newRdd = weightRdd.zip(dRdd) \n",
    "    wRdd = newRdd.map(lambda x : (x[0] - x[1]))\n",
    "    W = np.stack(wRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devX_Rdd = devRdd.zip(train_XRdd)\n",
    "#len(devZipX_Rdd.collect())\n",
    "# compute gradient : (predicted - y)X\n",
    "#gradRdd = devX_Rdd.map(lambda lst : ('keyA',compute_gradient(lst)))\n",
    "#dwRdd = gradRdd.reduceByKey(lambda x,y :  np.add(x,y))\n",
    "#gradientRdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradientRdd = dwRdd.mapValues(lambda dw : learning_rate * dw)\n",
    "#print(len(gradientRdd.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update weights\n",
    "#weightRdd = sc.parallelize(W)\n",
    "#devRdd.lookup('keyA')[0].shape\n",
    "#dRdd = sc.parallelize(gradientRdd.lookup('keyA')[0])\n",
    "#newRdd = weightRdd.zip(dRdd) \n",
    "#Rdd = newRdd.map(lambda x : (x[0] - x[1]))\n",
    "#type(np.stack(Rdd.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    " #   pred_Rdd =train_XRdd.map(lambda x : compute_z(x,W))\n",
    "  #  devRdd = pred_Rdd.zip(train_yRdd) \n",
    "   # devRdd = devRdd.map(lambda x : (x[0] - x[1]))\n",
    "    #devZipX_Rdd = devRdd.zip(train_XRdd)\n",
    "    #gradRdd = devZipX_Rdd.map(lambda lst : ('keyA',compute_gradient(lst)))\n",
    "    #gradientRdd = gradRdd.reduceByKey(lambda x,y :  np.add(x,y))\n",
    "    #devRdd = gradientRdd.mapValues(lambda dw : learning_rate * dw)\n",
    "    #weightRdd = sc.parallelize(W)\n",
    "    #devRdd.lookup('keyA')[0].shape\n",
    "    #dRdd = sc.parallelize(devRdd.lookup('keyA')[0])\n",
    "    #newRdd = weightRdd.zip(dRdd) \n",
    "    #newRdd = newRdd.map(lambda x : (x[0] - x[1]))\n",
    "    #W = np.stack(newRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.40765740e+03]\n",
      " [ -3.41301607e+02]\n",
      " [ -1.70757715e+02]\n",
      " [  2.08069351e+02]\n",
      " [ -3.04995139e+02]\n",
      " [ -2.41508316e+02]\n",
      " [ -1.29715350e+02]\n",
      " [ -1.46902387e+02]\n",
      " [  2.43532464e+02]\n",
      " [ -3.53817971e+02]\n",
      " [  3.75475811e+02]\n",
      " [  1.34805164e+01]\n",
      " [ -7.18716548e+01]\n",
      " [ -4.93317589e+02]\n",
      " [  1.52425748e+02]\n",
      " [ -1.46453560e+02]\n",
      " [  1.29718205e+01]\n",
      " [ -8.38421524e+02]\n",
      " [ -9.81356992e+01]\n",
      " [ -1.01056438e+02]\n",
      " [  9.00029550e+00]\n",
      " [ -1.12910711e+02]\n",
      " [ -5.22236627e+01]\n",
      " [ -2.09494257e+01]\n",
      " [ -5.60247604e+01]\n",
      " [  2.61633218e+00]\n",
      " [ -8.09291324e+00]\n",
      " [ -1.24621655e+01]\n",
      " [ -1.17391557e+01]\n",
      " [  8.97522452e+00]\n",
      " [ -2.42650908e+01]\n",
      " [ -1.47979654e+01]\n",
      " [ -2.52612922e+01]\n",
      " [ -5.74999825e+00]\n",
      " [ -7.45018467e+01]\n",
      " [ -1.75344707e+00]\n",
      " [  6.12502377e+00]\n",
      " [  1.10000000e+01]\n",
      " [  0.00000000e+00]\n",
      " [  1.97066505e+03]\n",
      " [  1.41418479e+03]\n",
      " [ -1.02120215e+03]\n",
      " [ -1.25185335e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for x in train_XRdd.collect():\n",
    "  \n",
    " #   x = np.append([1],[x])\n",
    "    #print(x_vector)\n",
    "  #  z=np.dot(x,W)\n",
    "   # print(sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Xm = np.column_stack((np.ones(len(test_data)), test_data.iloc[:,:-1]))\n",
    "test_y = test_data.iloc[:,-1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = test_data.iloc[:,:-1].as_matrix()\n",
    "test_y = test_data.iloc[:,-1].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_XRdd = sc.parallelize(test_X)\n",
    "test_yRdd = sc.parallelize(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Rdd =test_XRdd.map(lambda x : compute_h(x,W))\n",
    "pred_y = pred_Rdd.collect()\n",
    "#pred_y = compute_pred(test_Xm,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  59.95677531 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", 100 - np.mean(np.abs(pred_y - test_y)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
